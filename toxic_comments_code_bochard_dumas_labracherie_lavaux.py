# -*- coding: utf-8 -*-
"""Toxic_Comments_Code_Bochard_Dumas_Labracherie_Lavaux.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BGRg8bGzSKx8fBzYkJvHCFidiW2H36f4

BOCHARD-DUMAS-LABRACHERIE-LAVAUX PROJECT
"""

# Library imports

# Standard library
import re
import time
import random
import unicodedata

# NumPy / pandas
import numpy as np
import pandas as pd

# Visualization
import matplotlib.pyplot as plt

# NLTK (tokenization, stopwords, stemming)
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# Ensure required NLTK data is available
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')

# scikit-learn: splitting, features, models, metrics, utilities
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.utils.class_weight import compute_class_weight
from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    f1_score,
    roc_curve,
    roc_auc_score,
    average_precision_score,
    precision_recall_curve,
    auc,
)

# PyTorch (DL building blocks)
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch import amp

# Transformers (Hugging Face)
from transformers import (
    DistilBertForSequenceClassification,
    DistilBertTokenizerFast,
    DataCollatorWithPadding,
    get_linear_schedule_with_warmup,
)
import hashlib
import warnings

# Load dataset

data_comments = pd.read_csv("train.csv")

# Keep only the three relevant columns (id, text, label)
data_comments = data_comments[["id", "comment_text", "toxic"]]

# Display a quick preview of the data
print(data_comments.head())
print(data_comments.columns)

# Class distribution
print("\nClass distribution:")
print(data_comments["toxic"].value_counts())

"""## PART 1: TEXT PREPROCESSING ##"""

# Stopwords: keep negations and second person (they carry sentiment/polarity)
KEEP = {"not", "no", "nor", "n't", "never", "you", "your", "ur", "u", "youre"}
STOP = set(stopwords.words('english'))
STOP -= KEEP

# Deobfuscation mapping
# NOTE: We do NOT map '!' globally (to avoid "!!!" -> "iii").
#       We only map '!' to 'i' when it appears BETWEEN letters (e.g., b!tch -> bitch).
DEOB = str.maketrans({
    '@': 'a', '4': 'a',
    '1': 'i', '|': 'i',
    '3': 'e',
    '0': 'o',
    '$': 's', '5': 's',
    '7': 't',
    '€': 'e', '£': 'l'
})

EMAIL_RE   = re.compile(r'(?i)\b[a-z0-9._%+-]+@[a-z0-9.-]+\.[a-z]{2,}\b')
MENTION_RE = re.compile(r'(?<!\S)@\w+')

def normalize_repeats(token: str, max_repeat: int = 3) -> str:
    """Compress runs of the same char to at most `max_repeat` (e.g., sooooo -> sooo)."""
    return re.sub(r'(.)\1{' + str(max_repeat) + r',}', lambda m: m.group(1) * max_repeat, token)

def preprocess_text(text: str) -> str:
    """Toxicity preprocessing (EN) — keeps '!!' and '??', no forced ALLCAPS demotion."""
    if not isinstance(text, str):
        text = str(text)

    # 1) Unicode normalization
    t = unicodedata.normalize('NFKC', text)

    # 2) Remove URLs
    t = re.sub(r'https?://\S+|www\.\S+', ' ', t)

    # 3) Remove emails
    t = EMAIL_RE.sub(' ', t)

    # 4) Remove @mentions
    t = MENTION_RE.sub(' ', t)

    # 4-bis) PROTECT punctuation runs (before further cleaning)
    t = re.sub(r'!{2,}', ' __EXCL__ ', t)
    t = re.sub(r'\?{2,}', ' __QUEST__ ', t)

    # 5) Deobfuscation (e.g., '@'->'a', '1'->'i', etc.)
    t = t.translate(DEOB)

    # 5-bis) Map '!' to 'i' only when between letters (b!tch -> bitch)
    t = re.sub(r'(?i)(?<=[a-z])!(?=[a-z])', 'i', t)

    # 6) Punctuation cleanup to avoid glued tokens
    #    - remove solitary '!' or '?' (runs are preserved via placeholders)
    t = re.sub(r'(?<![!?])[!?](?![!?])', ' ', t)
    #    - replace .,;: with spaces
    t = re.sub(r'[.,;:]+', ' ', t)

    # 7) Tokenization
    toks = word_tokenize(t)

    out = []
    for tok in toks:
        raw_tok = tok  # keep original case

        # A) Restore protected runs
        if raw_tok == "__EXCL__":
            out.append("!!");  continue
        if raw_tok == "__QUEST__":
            out.append("??");  continue

        # B) Token cleanup (allow A–Z to preserve true ALLCAPS)
        tok = re.sub(r"<.*?>", " ", raw_tok)
        tok = re.sub(r"&[a-z]+;", " ", tok)
        tok = re.sub(r"[^A-Za-z']+", "", tok)
        tok = tok.replace("'", "")

        if not tok or '@' in tok:
            continue

        # C) Normalize character repeats (FUUUUCK -> FUUU, sooooo -> sooo)
        tok = normalize_repeats(tok, max_repeat=3)
        if not tok:
            continue

        # D) Stopwords (case-insensitive), with whitelist preserved above
        if tok.lower() in STOP:
            continue

        # E) Output: keep real ALLCAPS (len>1), else lowercase
        out.append(tok if (tok.isupper() and len(tok) > 1) else tok.lower())

    return " ".join(out)

# Apply preprocessing
data_comments["processed_text"] = data_comments["comment_text"].astype(str).apply(preprocess_text)

# Quick checks
print("Preprocessing preview:")
print(data_comments[["comment_text", "processed_text"]].head(8))

print("\nClass balance (normalized):")
print(
    data_comments["toxic"]
    .value_counts(normalize=True)
    .rename({0: "non-toxic", 1: "toxic"})
)

# Verifications

# 1) No more emails in our processed data set ?
EMAIL_RE = re.compile(r'(?i)\b[a-z0-9._%+-]+@[a-z0-9.-]+\.[a-z]{2,}\b')
has_email_after = data_comments["processed_text"].astype(str).str.contains(EMAIL_RE).sum()
print("Emails restants dans processed_text :", has_email_after)

# 2) No more '@' résiduel ?
has_at = data_comments["processed_text"].astype(str).str.contains(r"@").sum()
print("Caractère @ restant :", has_at)

# 3) Deob well normalised ?
tests = [
    "b1tch", "b!tch", "a55hole", "c@t", "sh1t", "You are NOT dumb!!!", "@john thanks???",
    "email me at x@y.com please"
]
for s in tests:
    print(s, "→", preprocess_text(s))

# 4) Verify that 'not' and 'you' are kept, 'the' removed
assert "not"   in preprocess_text("This is not good")
assert "you"   in preprocess_text("You are nice")
assert "the"   not in preprocess_text("the cat")

"""## PART 2: FEATURE ENGINEERING ##

Our text is now cleaned and pre-processed.  Next step: turn words into numbers that a model can use.

We’ll explore three common approaches:
1. **Bag of Words (BoW)**  
2. **N-grams**  
3. **TF-IDF**
"""

# BoW (no split) — dimensionality & sparsity

# Token pattern:
# - alphabetic words: [A-Za-z]+
# - emphasis punctuation runs: [!?]{2,} (keeps "!!", "??", "!!!", ...)
TOKEN_PATTERN = r"(?u)(?:[A-Za-z]+|[!?]{2,})"

# N-gram configurations to compare
ngram_ranges = {
    "Unigram (1,1)": (1, 1),
    "Bigram  (1,2)": (1, 2),
    "Trigram (1,3)": (1, 3),
}

print("=== BoW over the FULL corpus (exploration) ===")
for name, ngram in ngram_ranges.items():
    vec = CountVectorizer(
        ngram_range=ngram,
        lowercase=False,
        token_pattern=TOKEN_PATTERN
    )
    X_bow = vec.fit_transform(data_comments["processed_text"])
    n_docs, n_feats = X_bow.shape
    nnz = X_bow.count_nonzero()
    sparsity = 1.0 - (nnz / (n_docs * n_feats))

    print(
        f"{name}: shape={X_bow.shape} | "
        f"vocab={n_feats:,} | nnz={nnz:,} | sparsity={sparsity*100:.2f}%"
    )

# Data

X_text = data_comments["processed_text"]
y = data_comments["toxic"]

# Stratified split (important for class imbalance)
X_train, X_test, y_train, y_test = train_test_split(
    X_text, y, test_size=0.2, random_state=42, stratify=y
)

# Vectorizer configs to test (duplicates removed)
vectorizer_configs = [
    {'max_features':  50_000,   'ngram_range': (1, 2), 'min_df': 5, 'max_df': 0.80},
    {'max_features': 100_000,   'ngram_range': (1, 2), 'min_df': 5, 'max_df': 0.80},
    {'max_features': 150_000,   'ngram_range': (1, 2), 'min_df': 3, 'max_df': 0.85},
    {'max_features': 150_000,   'ngram_range': (1, 2), 'min_df': 1, 'max_df': 0.85},
    {'max_features': 1_500_000, 'ngram_range': (1, 2), 'min_df': 1, 'max_df': 0.85},
    {'max_features': 2_000_000, 'ngram_range': (1, 2), 'min_df': 1, 'max_df': 0.85},
]

results = []

print("=== TF-IDF + LinearSVC over stratified split ===")
for i, cfg in enumerate(vectorizer_configs, start=1):
    print(f"\n--- Test Configuration {i} ---\n{cfg}")
    pipe = Pipeline([
        ("tfidf", TfidfVectorizer(
            max_features=cfg['max_features'],
            ngram_range=cfg['ngram_range'],
            min_df=cfg['min_df'],
            max_df=cfg['max_df'],
            sublinear_tf=True,
            lowercase=False,
            token_pattern=TOKEN_PATTERN,
            dtype=np.float32
        )),
        ("clf", LinearSVC(random_state=42, class_weight="balanced"))
    ])

    # Fit and evaluate
    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)
    f1w = f1_score(y_test, y_pred, average="weighted")
    f1m = f1_score(y_test, y_pred, average="macro")

    results.append({
        "Config": i,
        **cfg,
        "F1_weighted": round(f1w, 4),
        "F1_macro": round(f1m, 4),
    })
    print(f"F1-weighted: {f1w:.4f} | F1-macro: {f1m:.4f}")

# Summary table + best configuration
df_res = pd.DataFrame(results).sort_values(by=["F1_weighted", "F1_macro"], ascending=False)
print("\n=== Results ===")
print(df_res)

best_row = df_res.iloc[0]
print("\nBest TF-IDF configuration:")
print(best_row.to_dict())

"""## PART 3: MODELING ##


We now have pre-processed the text and produced features. We can now move on to the modeling phase.
"""

# EDA (on TRAIN) + Lexical Baseline

# 0) Data — already preprocessed (space-separated tokens)
X = data_comments["processed_text"].astype(str).values
y = data_comments["toxic"].astype(int).values

X_tr, X_te, y_tr, y_te = train_test_split(
    X, y, test_size=0.20, random_state=42, stratify=y
)
print(f"Split -> TRAIN: {len(X_tr)} | TEST: {len(X_te)}")

# Words to ignore when selecting discriminative tokens (too generic / dataset-specific)
BAN = {"you", "your", "not", "nt", "wikipedia", "page", "user", "say", "one", "time"}

def compute_top_ngrams(
    texts, y, ngram_range=(1, 1), min_df=5, max_df=0.995,
    min_total_df=30, log_ratio_thr=0.7, ban=(),
    extra_filter=None, top_k=100
):
    """
    Compute a table of discriminative n-grams (by log-ratio) on TRAIN.
    - extra_filter(token:str)->bool can further filter candidates (e.g., "looks profane").
    """
    vec = CountVectorizer(tokenizer=str.split, lowercase=False,
                          ngram_range=ngram_range, min_df=min_df, max_df=max_df)
    Xtr = vec.fit_transform(texts)
    vocab = np.array(vec.get_feature_names_out())

    # Binarize per doc, then sum by class
    X_bin = (Xtr > 0).astype(np.int32)
    mask_pos = (y == 1)
    mask_neg = ~mask_pos
    n_pos, n_neg = int(mask_pos.sum()), int(mask_neg.sum())

    df_pos = np.asarray(X_bin[mask_pos].sum(axis=0)).ravel()
    df_neg = np.asarray(X_bin[mask_neg].sum(axis=0)).ravel()
    total_df = df_pos + df_neg

    # Discrimination scores
    log_ratio = np.log((df_pos + 1) / (df_neg + 1))
    prop_diff = (df_pos / max(n_pos, 1)) - (df_neg / max(n_neg, 1))

    # Selection masks
    keep = (total_df >= min_total_df) & (log_ratio >= log_ratio_thr)
    if ban:
        keep &= ~np.isin(vocab, list(ban))
    if extra_filter is not None:
        keep &= np.array([extra_filter(t) for t in vocab])

    df = pd.DataFrame({
        "token": vocab[keep],
        "df_tox": df_pos[keep],
        "df_non": df_neg[keep],
        "total_df": total_df[keep],
        "log_ratio": log_ratio[keep],
        "prop_diff": prop_diff[keep],
    }).sort_values(["log_ratio", "total_df"], ascending=[False, False])

    return df.head(top_k), vec

def looks_profane(tok: str) -> bool:
    # Heuristic cue list for profanity / insults (substring match, lowercased)
    cues = [
        "fuck","shit","bitch","cunt","ass","dick","fag","nigg","cock","suck",
        "whore","bastard","idiot","stupid","moron"
    ]
    return any(c in tok for c in cues)

def build_lexicon(top_uni_df, top_bi_df, max_uni=60, max_bi=30):
    """
    Build a lexical list of toxic cues from top unigrams/bigrams:
    - Unigrams: keep those that 'look profane'.
    - Bigrams: keep the most discriminative with a light cue filter.
    """
    # Unigrams: filter by heuristic
    lex_uni = set(
        top_uni_df.loc[top_uni_df["token"].map(looks_profane), "token"]
        .head(max_uni).tolist()
    )
    # Bigrams: cue-based filter + a few hard-coded patterns
    bi_keep = []
    cues = ["fuck", "shut", "go", "kill", "die", "screw", "piece"]
    for t in top_bi_df["token"].head(max_bi).tolist():
        if any(c in t for c in cues):
            bi_keep.append(t)
    lex_bi = set(bi_keep) | {"fuck you", "shut up", "go die", "kill yourself"}
    return lex_uni, lex_bi

def baseline_scores_and_preds(texts, lex_uni, lex_bi):
    """
    Simple lexical baseline:
    - Predict 1 (toxic) if any unigram/bigram from the lexicon is present.
    - Also returns a continuous score = number of hits (for ROC/PR).
    """
    scores, preds = [], []
    uni_only = {w for w in lex_uni if " " not in w}
    bi_only  = {w for w in lex_bi if " " in w}

    for s in texts:
        s_l = f" {s.lower()} "
        toks = set(s_l.split())

        hit_uni = bool(toks & uni_only)
        hit_bi  = any(p in s_l for p in bi_only)
        hit = hit_uni or hit_bi
        preds.append(1 if hit else 0)

        sc = sum((w in toks) for w in uni_only) + sum((p in s_l) for p in bi_only)
        scores.append(sc)

    return np.array(scores), np.array(preds)

# -------- 1) EDA on TRAIN --------
top_uni, _ = compute_top_ngrams(
    X_tr, y_tr, ngram_range=(1, 1),
    min_df=5, max_df=0.995,
    min_total_df=30, log_ratio_thr=0.7,
    ban=BAN, extra_filter=None, top_k=100
)
print("\n[EDA] Top toxic unigrams (TRAIN):")
print(top_uni.head(20))

top_bi, _ = compute_top_ngrams(
    X_tr, y_tr, ngram_range=(2, 2),
    min_df=10, max_df=0.995,
    min_total_df=10, log_ratio_thr=0.7,
    ban=(), extra_filter=None, top_k=50
)
print("\n[EDA] Top toxic bigrams (TRAIN):")
print(top_bi.head(15))

# -------- 2) Lexicon baseline from TRAIN --------
LEX_UNI, LEX_BI = build_lexicon(top_uni, top_bi, max_uni=60, max_bi=30)
print("\nLexicon — unigrams (sample):", sorted(list(LEX_UNI))[:12], f"... (total {len(LEX_UNI)})")
print("Lexicon — bigrams  (sample):", sorted(list(LEX_BI))[:8],  f"... (total {len(LEX_BI)})")

# -------- 3) Baseline on TEST --------
scores_base, y_pred_base = baseline_scores_and_preds(X_te, LEX_UNI, LEX_BI)

print("\n=== Lexical baseline (learned on TRAIN) — TEST ===")
print(classification_report(y_te, y_pred_base, target_names=["non-toxic","toxic"]))
print("Confusion matrix:\n", confusion_matrix(y_te, y_pred_base))
print(
    f"F1-weighted: {f1_score(y_te, y_pred_base, average='weighted'):.4f} | "
    f"F1-macro: {f1_score(y_te, y_pred_base, average='macro'):.4f}"
)
print(
    f"ROC-AUC: {roc_auc_score(y_te, scores_base):.4f} | "
    f"PR-AUC (toxic): {average_precision_score(y_te, scores_base):.4f}"
)

def plot_lexicon_bars(df, lexicon, title, top_n=20, metric="log_ratio",
                      bar_height=0.30, left_margin=0.32):

    sel = df[df["token"].isin(lexicon)].copy()
    sel = sel.sort_values(metric, ascending=False).head(top_n)

    height = max(4, 0.45 * len(sel))
    fig, ax = plt.subplots(figsize=(10, height))

    ax.barh(sel["token"][::-1], sel[metric][::-1],
            height=bar_height,
            edgecolor="black", linewidth=0.4)

    ax.set_xlabel(metric.replace("_", " ").title())
    ax.set_title(title)
    fig.tight_layout()
    fig.subplots_adjust(left=left_margin)
    plt.show()

plot_lexicon_bars(top_uni, LEX_UNI, "Top lexicon unigrams by log-ratio", top_n=12, bar_height=0.28)
plot_lexicon_bars(top_bi,  LEX_BI,  "Top lexicon bigrams by log-ratio", top_n=12, bar_height=0.25)

print("UNIGRAMS:", sorted(LEX_UNI))
print("BIGRAMS :", sorted(LEX_BI))

# Logistic Regression with TF-IDF (Config #6)

# 1) Split on TEXT (avoid leakage by fitting vectorizer only on TRAIN)
X_text = data_comments["processed_text"]
y = data_comments["toxic"]

X_train_text, X_test_text, y_train, y_test = train_test_split(
    X_text, y, test_size=0.20, random_state=42, stratify=y
)

# 2) TF-IDF (Config #6) — fit on TRAIN only

vectorizer_tfidf = TfidfVectorizer(
    max_features=2_000_000,
    ngram_range=(1, 2),
    min_df=1,
    max_df=0.85,
    lowercase=False,

    dtype=np.float32
)

X_train = vectorizer_tfidf.fit_transform(X_train_text)
X_test  = vectorizer_tfidf.transform(X_test_text)
print("Train shape:", X_train.shape, " | Test shape:", X_test.shape)

# 3) Logistic Regression for high-dimensional sparse features

model_lr = LogisticRegression(
    max_iter=400,
    solver="saga",
    class_weight="balanced",
    n_jobs=-1,
    random_state=42
)
model_lr.fit(X_train, y_train)

# 4) Evaluation on TEST
y_pred_test_lr  = model_lr.predict(X_test)
y_proba_test_lr = model_lr.predict_proba(X_test)[:, 1]

print("F1-weighted:", round(f1_score(y_test, y_pred_test_lr, average="weighted"), 4))
print("F1-macro   :", round(f1_score(y_test, y_pred_test_lr, average="macro"), 4))
print("\nClassification report:\n", classification_report(y_test, y_pred_test_lr, digits=3))
print("ROC AUC    :", round(roc_auc_score(y_test, y_proba_test_lr), 4))

# Logistic Regression with TF-IDF (Config #6) — improved

# 1) Split ON TEXT (avoid leakage by fitting the vectorizer only on TRAIN)
X_text = data_comments["processed_text"]
y = data_comments["toxic"]

X_train_text, X_test_text, y_train, y_test = train_test_split(
    X_text, y, test_size=0.20, random_state=42, stratify=y
)

# 2) TF-IDF (config #6) — fit on TRAIN only
vectorizer_tfidf = TfidfVectorizer(
    max_features=2_000_000,
    ngram_range=(1, 2),
    min_df=1,
    max_df=0.85,
    lowercase=False,
)

X_train = vectorizer_tfidf.fit_transform(X_train_text)
X_test  = vectorizer_tfidf.transform(X_test_text)
print("Train shape:", X_train.shape, " | Test shape:", X_test.shape)

# 3) Logistic Regression tuned for high-dimensional sparse data
#    - class_weight='balanced' mitigates class imbalance
#    - consider solver='saga' and n_jobs=-1 for very large vocabularies
model_lr = LogisticRegression(
    max_iter=400,
    class_weight="balanced",
    random_state=42
)
model_lr.fit(X_train, y_train)

# 4) Evaluation
y_pred_test_lr  = model_lr.predict(X_test)
y_proba_test_lr = model_lr.predict_proba(X_test)[:, 1]

print("F1-weighted:", round(f1_score(y_test, y_pred_test_lr, average="weighted"), 4))
print("F1-macro   :", round(f1_score(y_test, y_pred_test_lr, average="macro"), 4))
print("\nClassification report:\n", classification_report(y_test, y_pred_test_lr, digits=3))
print("ROC AUC    :", round(roc_auc_score(y_test, y_proba_test_lr), 4))

# LinearSVC model

# Train
model_svc = LinearSVC(
    random_state=42,
    max_iter=4000,
    class_weight="balanced"
)
model_svc.fit(X_train, y_train)

# Predictions
y_pred_test_svc = model_svc.predict(X_test)
y_scores_svc = model_svc.decision_function(X_test)

# Evaluation
print("F1-weighted:", round(f1_score(y_test, y_pred_test_svc, average="weighted"), 4))
print("F1-macro   :", round(f1_score(y_test, y_pred_test_svc, average="macro"), 4))
print("\nClassification report:\n", classification_report(y_test, y_pred_test_svc, digits=3))
print("ROC AUC    :", round(roc_auc_score(y_test, y_scores_svc), 4))

# DistilBERT

# Config & seeding
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# Main settings
FREEZE_BERT = False
EPOCHS = 2
BATCH_TRAIN = 16
BATCH_EVAL  = 64
MAX_LEN = 128

POLICY = "balanced"

# Data loading
# Use data_comments if available; otherwise fallback to df
DF = data_comments if 'data_comments' in globals() else df
assert {'comment_text','toxic'}.issubset(DF.columns), "Columns 'comment_text' and 'toxic' (0/1) are required."

# Clean labels (0/1) & text (emails/mentions)
DF = DF[['comment_text', 'toxic']].copy()
DF = DF[DF['comment_text'].notna()].copy()

# Map string labels to binary if needed
if DF['toxic'].dtype == 'object':
    map_dict = {'toxic':1, 'non-toxic':0, 'yes':1, 'no':0, 'true':1, 'false':0}
    DF['toxic'] = DF['toxic'].astype(str).str.strip().str.lower().map(map_dict)

lab = pd.to_numeric(DF['toxic'], errors='coerce')
mask_finite = np.isfinite(lab)
DF = DF.loc[mask_finite].copy()
lab = lab.loc[mask_finite]
if not set(np.unique(lab.dropna().round().astype(int))).issubset({0,1}):
    lab = (lab >= 0.5).astype(int)
else:
    lab = lab.astype(int)
DF['toxic'] = lab

# Remove emails / @mentions
EMAIL_RE   = re.compile(r"(?i)\b[a-z0-9._%+-]+@[a-z0-9.-]+\.[a-z]{2,}\b")
MENTION_RE = re.compile(r"(?<!\S)@\w+")
def sanitize_text(s: str) -> str:
    s = str(s)
    s = EMAIL_RE.sub(" ", s)
    s = MENTION_RE.sub(" ", s)
    return s

texts  = DF["comment_text"].astype(str).map(sanitize_text).tolist()
labels = DF["toxic"].astype(int).tolist()

print("Label distribution:", pd.Series(labels).value_counts(normalize=True).round(3).to_dict())

# Stratified split: train/test then train/valid
X_train_text, X_test_text, y_train_all, y_test = train_test_split(
    texts, labels, test_size=0.2, random_state=SEED, stratify=labels
)
X_tr_text, X_va_text, y_tr, y_va = train_test_split(
    X_train_text, y_train_all, test_size=0.15, random_state=SEED, stratify=y_train_all
)
print(f"Train: {len(X_tr_text)} | Valid: {len(X_va_text)} | Test: {len(X_test_text)}")

# Tokenizer & Datasets
tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")

class ToxicDataset(Dataset):
    def __init__(self, texts, labels, tok, max_length=128):
        self.texts, self.labels, self.tok, self.max_length = texts, labels, tok, max_length
    def __len__(self): return len(self.labels)
    def __getitem__(self, idx):
        enc = self.tok(
            self.texts[idx],
            truncation=True,
            max_length=self.max_length,
            padding=False,
            return_tensors=None
        )
        enc["labels"] = self.labels[idx]
        return enc

train_ds = ToxicDataset(X_tr_text, y_tr, tokenizer, max_length=MAX_LEN)
valid_ds = ToxicDataset(X_va_text, y_va, tokenizer, max_length=MAX_LEN)
test_ds  = ToxicDataset(X_test_text, y_test, tokenizer, max_length=MAX_LEN)

collate = DataCollatorWithPadding(
    tokenizer, pad_to_multiple_of=(8 if device.type=="cuda" else None)
)
train_loader = DataLoader(train_ds, batch_size=BATCH_TRAIN, shuffle=True,
                          collate_fn=collate, num_workers=2, pin_memory=(device.type=="cuda"))
valid_loader = DataLoader(valid_ds, batch_size=BATCH_EVAL, shuffle=False,
                          collate_fn=collate, num_workers=2, pin_memory=(device.type=="cuda"))
test_loader  = DataLoader(test_ds,  batch_size=BATCH_EVAL, shuffle=False,
                          collate_fn=collate, num_workers=2, pin_memory=(device.type=="cuda"))

# Model & optimization
model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased", num_labels=2
).to(device)

if FREEZE_BERT:
    for p in model.distilbert.parameters():
        p.requires_grad = False
    print("DistilBERT backbone frozen (training head only).")

# Class weights for imbalance
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_tr), y=y_tr)
class_weights = torch.tensor(class_weights, dtype=torch.float32, device=device)
loss_fn = nn.CrossEntropyLoss(weight=class_weights)

optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)

# Scheduler (10% warmup)
num_training_steps = len(train_loader) * EPOCHS
num_warmup_steps = int(0.1 * num_training_steps)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)

# AMP (mixed precision) when CUDA is available
use_amp = (device.type == "cuda")
scaler = amp.GradScaler('cuda' if device.type=='cuda' else 'cpu', enabled=use_amp)

# Training / eval loops
def run_epoch(dataloader, train=True):
    model.train(train)
    total = 0.0
    for batch in dataloader:
        input_ids = batch['input_ids'].to(device, non_blocking=True)
        attention_mask = batch['attention_mask'].to(device, non_blocking=True)
        labels = torch.as_tensor(batch['labels'], dtype=torch.long, device=device)

        if train:
            optimizer.zero_grad(set_to_none=True)

        with amp.autocast(device_type=device.type, enabled=use_amp):
            out = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = out.logits
            loss = loss_fn(logits, labels)

        if train:
            scaler.scale(loss).backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()

        total += loss.item()
    return total

def predict_proba(dataloader):
    model.eval()
    preds, probs, labs = [], [], []
    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device, non_blocking=True)
            attention_mask = batch['attention_mask'].to(device, non_blocking=True)
            labels = torch.as_tensor(batch['labels'], dtype=torch.long, device=device)

            with amp.autocast(device_type=device.type, enabled=use_amp):
                logits = model(input_ids=input_ids, attention_mask=attention_mask).logits
                prob = torch.softmax(logits, dim=1)[:, 1]

            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())
            probs.extend(prob.cpu().numpy())
            labs.extend(labels.cpu().numpy())
    return np.array(preds), np.array(probs), np.array(labs)

# Training
print("=== Training DistilBERT ===")
t0 = time.time()
for ep in range(1, EPOCHS+1):
    tr_loss = run_epoch(train_loader, train=True)
    model.eval()
    with torch.no_grad():
        va_loss = run_epoch(valid_loader, train=False)
    print(f"Epoch {ep}/{EPOCHS} | train_loss={tr_loss:.2f} | valid_loss={va_loss:.2f}")
train_time = time.time() - t0
print(f"Training time: {train_time:.1f}s")

# Threshold tuning on VALID
_, probs_valid, y_valid = predict_proba(valid_loader)
prec, rec, thr = precision_recall_curve(y_valid, probs_valid)

if POLICY == "balanced":
    f1s = (2 * prec * rec) / (prec + rec + 1e-12)
    best_idx = np.nanargmax(f1s[1:])
    thr_use = thr[best_idx]
elif POLICY == "recall90":
    idx = np.where(rec[1:] >= 0.90)[0]
    thr_use = thr[idx[-1]] if len(idx) else 0.5
elif POLICY == "precision85":
    idx = np.where(prec[1:] >= 0.85)[0]
    thr_use = thr[idx[0]] if len(idx) else 0.5
else:
    thr_use = 0.5

print(f"Chosen threshold ({POLICY}): {thr_use:.3f}")

# Final TEST evaluation
preds_test, probs_test, y_test_arr = predict_proba(test_loader)

def report_with_threshold(thr_value, tag="custom threshold"):
    y_pred = (probs_test >= thr_value).astype(int)
    print(f"\n=== Report ({tag}) ===")
    print(classification_report(y_test_arr, y_pred, target_names=["non-toxic","toxic"]))
    print("Confusion matrix:\n", confusion_matrix(y_test_arr, y_pred))

# Report at tuned threshold + 0.5 reference
report_with_threshold(thr_use, f"policy={POLICY}")
report_with_threshold(0.5, "threshold 0.5 (reference)")

# AUC (threshold-invariant)
roc_auc = roc_auc_score(y_test_arr, probs_test)
pr_auc  = average_precision_score(y_test_arr, probs_test)
print(f"\nROC-AUC: {roc_auc:.4f} | PR-AUC (toxic): {pr_auc:.4f}")

"""## PART 4: EVALUATION ##"""

# Classification reports + AUC for all models

def show_results(name, y_true, y_pred, y_scores=None):
    """Pretty-print a model's classification report and optional ROC-AUC."""
    print(f"\n=== {name} ===")
    print(classification_report(y_true, y_pred, target_names=["non-toxic","toxic"], digits=3))
    if y_scores is not None:
        try:
            auc = roc_auc_score(y_true, y_scores)
            print(f"ROC-AUC: {auc:.4f}")
        except Exception as e:
            print("ROC-AUC not computable:", e)

# Lexical baseline
if all(v in globals() for v in ["y_te", "y_pred_base", "scores_base"]):
    show_results("Lexical baseline", y_te, y_pred_base, y_scores=scores_base)

# Logistic Regression
if all(v in globals() for v in ["y_test", "y_pred_test_lr", "y_proba_test_lr"]):
    show_results("Logistic Regression (TF-IDF, balanced)", y_test, y_pred_test_lr, y_scores=y_proba_test_lr)

# LinearSVC
if all(v in globals() for v in ["y_test", "y_pred_test_svc", "y_scores_svc"]):
    show_results("LinearSVC", y_test, y_pred_test_svc, y_scores=y_scores_svc)

# DistilBERT
if all(v in globals() for v in ["y_test_arr", "probs_test"]):
    # tuned-threshold policy (thr_use if available)
    if "thr_use" in globals():
        y_pred_policy = (probs_test >= thr_use).astype(int)
        show_results(f"DistilBERT (policy={POLICY})", y_test_arr, y_pred_policy, y_scores=probs_test)
    # fixed threshold 0.5
    y_pred_05 = (probs_test >= 0.5).astype(int)
    show_results("DistilBERT (threshold 0.5)", y_test_arr, y_pred_05, y_scores=probs_test)

# Text-only confusion matrices (counts) for LR, LinearSVC, DistilBERT


def show_cm_counts(name: str, y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])
    print(f"\n{name}")
    print("Confusion matrix (counts):")
    print(cm)

# Logistic Regression (needs: y_test, y_pred_test_lr)
if all(v in globals() for v in ["y_test", "y_pred_test_lr"]):
    show_cm_counts("Logistic Regression (TF-IDF, balanced)", y_test, y_pred_test_lr)

# LinearSVC (needs: y_test, y_pred_test_svc)
if all(v in globals() for v in ["y_test", "y_pred_test_svc"]):
    show_cm_counts("LinearSVC", y_test, y_pred_test_svc)

# DistilBERT — threshold 0.5 (needs: y_test_arr, probs_test)
if all(v in globals() for v in ["y_test_arr", "probs_test"]):
    y_pred_bert_05 = (probs_test >= 0.5).astype(int)
    show_cm_counts("DistilBERT (threshold 0.5)", y_test_arr, y_pred_bert_05)

    # DistilBERT — tuned policy threshold if available (needs: thr_use, POLICY)
    if "thr_use" in globals():
        y_pred_bert_policy = (probs_test >= thr_use).astype(int)
        show_cm_counts(f"DistilBERT (policy={POLICY})", y_test_arr, y_pred_bert_policy)

# === ROC Curves

PAD = 0.03

candidates = []

# Baseline lexicale (scores_base = nb de hits, y_te = labels TEST baseline)
if all(v in globals() for v in ["y_te", "scores_base"]):
    candidates.append(("Baseline lexicale", np.asarray(y_te), np.asarray(scores_base)))

# Logistic Regression (y_test, y_proba_test_lr)
if all(v in globals() for v in ["y_test", "y_proba_test_lr"]):
    candidates.append(("Logistic Regression (TF-IDF, balanced)", np.asarray(y_test), np.asarray(y_proba_test_lr)))

# LinearSVC (y_test, y_scores_svc = decision_function)
if all(v in globals() for v in ["y_test", "y_scores_svc"]):
    candidates.append(("LinearSVC", np.asarray(y_test), np.asarray(y_scores_svc)))

# DistilBERT (y_test_arr, probs_test)
if all(v in globals() for v in ["y_test_arr", "probs_test"]):
    candidates.append(("DistilBERT", np.asarray(y_test_arr), np.asarray(probs_test)))

clean = []
for name, y_true, scores in candidates:
    try:
        y_true = y_true.astype(int)
        if y_true.ndim != 1 or scores.ndim != 1 or len(y_true) != len(scores):
            continue
        if not set(np.unique(y_true)).issubset({0,1}):
            continue
        clean.append((name, y_true, scores))
    except Exception:
        pass

if not clean:
    raise RuntimeError("Aucun couple (y_true, scores) valide trouvé. Vérifie les variables des cellules amont.")

def signature(y):
    h = hashlib.md5(y.tobytes()).hexdigest()
    return (len(y), h)

groups = {}
for name, y_true, scores in clean:
    groups.setdefault(signature(y_true), []).append((name, y_true, scores))

if len(groups) > 1:
    warnings.warn("Plusieurs jeux de test détectés (baseline / TF-IDF / BERT). "
                  "Les courbes sont tracées par split pour éviter une comparaison biaisée.")

# ---- Trace une figure ROC + PR par split ----
for gidx, ((n, _), models) in enumerate(groups.items(), start=1):
    y_any = models[0][1]
    n_pos = int((y_any == 1).sum())
    n_neg = int((y_any == 0).sum())

    # ===== ROC =====
    plt.figure(figsize=(6.5, 5))
    for name, y_true, scores in models:
        fpr, tpr, _ = roc_curve(y_true, scores)
        aucR = roc_auc_score(y_true, scores)
        plt.plot(fpr, tpr, lw=2, label=f"{name} (AUC={aucR:.4f})")

    plt.plot([-PAD, 1+PAD], [-PAD, 1+PAD], linestyle="--", lw=1, color="red")

    plt.xlim([-PAD, 1+PAD]); plt.ylim([-PAD, 1+PAD])
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(f"ROC — Split #{gidx} (N={n}, pos={n_pos}, neg={n_neg})")
    plt.legend(loc="lower right")
    plt.grid(alpha=0.25)
    plt.show()

    # ===== PR  =====
    plt.figure(figsize=(6.5, 5))
    for name, y_true, scores in models:
        prec, rec, _ = precision_recall_curve(y_true, scores)
        ap = average_precision_score(y_true, scores)
        plt.plot(rec, prec, lw=2, label=f"{name} (AP={ap:.4f})")
    # baseline no-skill = proportion de positifs
    pos_rate = n_pos / max(n_pos + n_neg, 1)
    plt.hlines(pos_rate, -PAD, 1+PAD, linestyles="--", color="red")
    plt.xlim([-PAD, 1+PAD]); plt.ylim([-PAD, 1+PAD])
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title(f"Precision–Recall — Split #{gidx} (pos rate={pos_rate:.3f})")
    plt.legend(loc="upper right")
    plt.grid(alpha=0.25)
    plt.show()

